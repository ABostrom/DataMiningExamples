{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup imports for numpy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "\n",
    "#In this example I am using one of the built in datasets\n",
    "#Some classification problems will give you pre defined train and test splits\n",
    "#in this case, as we don't have a train test split i make one using the functionality in sklearn\n",
    "\n",
    "#here i am using the built in datasets in sklearn.\n",
    "from sklearn import datasets\n",
    "\n",
    "#load the iris datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data   #setup the tabular data\n",
    "y = iris.target #setup the labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into train and test splits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#using train test split, i take the data and labels, and split them into a\n",
    "#80/20 train test set. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#could pre-process the data\n",
    "\n",
    "#in scit kit learn we have access to some pre-processing steps\n",
    "\n",
    "#here i am going to normalise the attributes in our train and test set\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#IMPORTANT: Whatever pre-processing steps you perform on the test data, you must do on the train data\n",
    "#It's considered good practice to perform pre-processing steps independently from the train and test\n",
    "#as this mirrors a real scenario.\n",
    "\n",
    "#in this case, this performs L2 norm, column wise.\n",
    "X_train_norm = normalize(X_train) \n",
    "X_test_norm = normalize(X_test)\n",
    "\n",
    "#if we normalised the entire data set and then split we would have slightly different norm values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup your nearest classifier. \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Here i create a 2NN classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "#fit the classifier on our normalised train data\n",
    "#and our train labels!\n",
    "clf.fit(X_train_norm, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect predictions on test data\n",
    "y_hat = clf.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the score - accuracy / error or other metrics.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ]
}