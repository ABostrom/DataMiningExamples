{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0a6b797a4091570d69f8a8160f81f5dba34294d1e3a310d498be04ef5196ed8f6",
   "display_name": "Python 3.7.9 64-bit ('DeepRL': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Heart Disease UCI\n",
    "\n",
    "subset of the 74 attributes in the original: https://archive.ics.uci.edu/ml/datasets/heart+disease\n",
    "\n",
    "Attribute Information:\n",
    "- age\n",
    "- sex\n",
    "- chest pain type (4 values)\n",
    "- resting blood pressure\n",
    "- serum cholestoral in mg/dl\n",
    "- fasting blood sugar > 120 mg/dl\n",
    "- resting electrocardiographic results (values 0,1,2)\n",
    "- maximum heart rate achieved\n",
    "- exercise induced angina\n",
    "- oldpeak = ST depression induced by exercise relative to rest\n",
    "- the slope of the peak exercise ST segment\n",
    "- number of major vessels (0-3) colored by flourosopy\n",
    "- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We saw this dataset in week 6, where we learned how to use classifiers to train and predict labels for this dataset.\n",
    "\n",
    "Using ensembles can we improve upon the performance we had before?\n",
    "Do ensembles give us more robust solutions to this problem?\n",
    "\n",
    "Are heterogenous ensembles or homogenous problems better or worse?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n0     63    1   3       145   233    1        0      150      0      2.3   \n1     37    1   2       130   250    0        1      187      0      3.5   \n2     41    0   1       130   204    0        0      172      0      1.4   \n3     56    1   1       120   236    0        1      178      0      0.8   \n4     57    0   0       120   354    0        1      163      1      0.6   \n..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n298   57    0   0       140   241    0        1      123      1      0.2   \n299   45    1   3       110   264    0        1      132      0      1.2   \n300   68    1   0       144   193    1        1      141      0      3.4   \n301   57    1   0       130   131    0        1      115      1      1.2   \n302   57    0   1       130   236    0        0      174      0      0.0   \n\n     slope  ca  thal  \n0        0   0     1  \n1        0   0     2  \n2        2   0     2  \n3        2   0     2  \n4        2   0     2  \n..     ...  ..   ...  \n298      1   0     3  \n299      1   0     3  \n300      1   2     3  \n301      1   1     3  \n302      1   1     2  \n\n[303 rows x 13 columns]\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n298    0\n299    0\n300    0\n301    0\n302    0\nName: target, Length: 303, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#designate the path where you saved your OEC data\n",
    "heart_data_path = \"../../Datasets/heart.csv\"\n",
    "\n",
    "#Load the data using pandas read_csv function. \n",
    "orig_data = pd.read_csv(heart_data_path)\n",
    "\n",
    "#get the data out, leaving behind the target column.\n",
    "X = orig_data.iloc[:, :-1]\n",
    "#extract the target column.\n",
    "y = orig_data[\"target\"] \n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "Split the dataset into train and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "You may wish to train some scaling or preprocessing transformers. \n",
    "- Conisder the standard scaler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use a transformer to scale our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "build a number of individual classifiers to train and use on the data.\n",
    "\n",
    "I chose to build 4 standard classifiers - consider how and why you chose these classifiers for your problem space.\n",
    "\n",
    "What paramteres did you set and why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Write your own simple ensemble that picks the majority voted class.\n",
    "\n",
    "Similar to how we wrote a 1NN in the first classification lab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import *\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class MyEnsemble(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "\n",
    "    def __init__(self, classifiers):\n",
    "        # store the estimators to ensemble\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # check the data is correctly formatted.\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # fit each model\n",
    "\n",
    "\n",
    "        self.fitted_ =True\n",
    "\n",
    "        # return classifier\n",
    "        return self\n",
    "\n",
    "    # Note: YOU DO NOT NEED TO DO THIS USING FANCY NUMPY\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # check the data is valid.\n",
    "        X = check_array(X)\n",
    "\n",
    "        # gather predictions from classifiers\n",
    "        # find out which is the most common prediction for each data point\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "#fit and train your ensemble\n",
    "\n"
   ]
  },
  {
   "source": [
    "build a number of ensemble learners, using Bagging, GradientBoosting and AdaBoost.\n",
    "\n",
    "Consider the pramaters you use, and what base classifiers to set for these meta learners"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "If you used any transformers, create a pipeline of your classifiers and ensembles so that we can have consistent behaviour"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a number of pipelines to classify the dataset on.\n"
   ]
  },
  {
   "source": [
    "Fit each pipeline/classifier on the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "measure and store the performance and name of each model with respect to accuracy (can use other measures)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Print out the performance of each model\n",
    "\n",
    "here i stored the accuracies and names as a tuple in the form `[(clf_name_0, score_0),...,(clf_name_n, score_n)]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|Classifier                    |Accuracy|\n|SVC                           |0.8026\t|\n|RandomForestClassifier        |0.8026\t|\n|KNeighborsClassifier          |0.7763\t|\n|LogisticRegression            |0.8158\t|\n|BaggingClassifier             |0.7500\t|\n|GradientBoostingClassifier    |0.7763\t|\n|AdaBoostClassifier            |0.8026\t|\n|MyEnsemble                    |0.7895\t|\n"
     ]
    }
   ],
   "source": [
    "print(f\"|{'Classifier'.ljust(30)}|Accuracy|\")\n",
    "for name,acc in accs:\n",
    "    print(f\"|{name.ljust(30)}|{acc:.4f}\\t|\")"
   ]
  },
  {
   "source": [
    "Just like other machine learning models - Ensembles need to be tuned to extract better performance, and fit parameters.\n",
    "\n",
    "Tuning ensembles is a very complex topic - consider how you can tune some parameters of the ensemble\n",
    "\n",
    "Using a VotingClassifier, choose several models, and use GridSearchCV to tune one parameter for each estimator\n",
    "\n",
    "tune one parameter of the ensemble itself: Consider the voting scheme \"hard\"/\"soft\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "output the accuracy and the tuned parameters.\n",
    "\n",
    "Which model is best and why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 238,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|VotingClassifer               |0.7237\t|\n{'nn__n_neighbors': 8, 'rf__n_estimators': 200, 'svc__C': 0.1, 'voting': 'soft'}\n"
     ]
    }
   ]
  }
 ]
}